{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3).Eye_Gaze_Detection1\n",
    "\n",
    "### Why do we need to detect the gaze?\n",
    "The idea is to see a keyboard on the screen, light up each key every second or so and when the key weâ€™re insterested in is on, simply we close our eyes.\n",
    "\n",
    "If we consider that the keyboard has 26 letters of the alphabet, plus we need the space and some other key, just to run through all the keys lighting up each for one second, it will take half a mintue each time.\n",
    "\n",
    "The idea is to devide the keyboard in two parts. If we look on the left side only the left part of the keyboard will be activated, while if we look on the right side only the letter on the right part of the keyboard will light up.\n",
    "\n",
    "\n",
    "<img src=\"picture/keyboard.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import dlib\n",
    "from math import hypot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we used the detector to detect the frontal face\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# it will dectect the facial landwark points \n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "font = cv2.FONT_HERSHEY_PLAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We create a function that we will need later on to detect the medium point.\n",
    "#On this function we simply put the coordinates of two points and will return the medium point \n",
    "#(the points in the middle between the two points).\n",
    "def midpoint(p1 ,p2):\n",
    "    return int((p1.x + p2.x)/2), int((p1.y + p2.y)/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_blinking_ratio(eye_points, facial_landmarks):\n",
    "    \n",
    "    #to detect the left_side of a left eye\n",
    "    left_point = (facial_landmarks.part(eye_points[0]).x, facial_landmarks.part(eye_points[0]).y)\n",
    "    \n",
    "    #to detect the right_side of the left eye\n",
    "    right_point = (facial_landmarks.part(eye_points[3]).x, facial_landmarks.part(eye_points[3]).y)\n",
    "    \n",
    "    #to detect the mid point for the center of top in left eye\n",
    "    center_top = midpoint(facial_landmarks.part(eye_points[1]), facial_landmarks.part(eye_points[2]))\n",
    "    \n",
    "    #to detect the mid point for the center of the bottom in left eye\n",
    "    center_bottom = midpoint(facial_landmarks.part(eye_points[5]), facial_landmarks.part(eye_points[4]))\n",
    "    \n",
    "    #to calculate horizontal line distance\n",
    "    hor_line_lenght = hypot((left_point[0] - right_point[0]), (left_point[1] - right_point[1]))\n",
    "    \n",
    "    #to calculate vertical line distance\n",
    "    ver_line_lenght = hypot((center_top[0] - center_bottom[0]), (center_top[1] - center_bottom[1]))\n",
    "    \n",
    "    #to calculate ratio\n",
    "    ratio = hor_line_lenght / ver_line_lenght\n",
    "    \n",
    "    return ratio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detect gaze of left eye\n",
    "We need to detect the gaze of both eyes, but for the moment we will focus only on one eye and later we will apply the same method for the second eye.\n",
    "\n",
    "We can select the second eye simply taking the coordinates from the landmarks points.\n",
    "\n",
    "We know that the left eye region corresponds to the landmarks with indexes: 36, 37, 38, 39, 40 and 41, so we take them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to open webcab to capture the image\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    _,frame = cap.read()\n",
    "    \n",
    "    #change the color of the frame captured by webcam to grey\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # to detect faces from grey color frame\n",
    "    faces = detector(gray)\n",
    "    for face in faces:\n",
    "        \n",
    "        #to detect the landmarks of a face\n",
    "        landmarks = predictor(gray, face)\n",
    "        \n",
    "        left_eye_ratio = get_blinking_ratio([36, 37, 38, 39, 40, 41], landmarks)\n",
    "        right_eye_ratio = get_blinking_ratio([42, 43, 44, 45, 46, 47], landmarks)\n",
    "        blinking_ratio = (left_eye_ratio + right_eye_ratio) / 2\n",
    "\n",
    "    \n",
    "        if blinking_ratio>5.7:\n",
    "            cv2.putText(frame, \"BLINKING\", (50, 150), font, 7, (255, 0, 0))\n",
    "            \n",
    "            \n",
    "        # Gaze detection\n",
    "        #getting the area from the frame of the left eye only\n",
    "        left_eye_region = np.array([(landmarks.part(36).x, landmarks.part(36).y),\n",
    "                            (landmarks.part(37).x, landmarks.part(37).y),\n",
    "                            (landmarks.part(38).x, landmarks.part(38).y),\n",
    "                            (landmarks.part(39).x, landmarks.part(39).y),\n",
    "                            (landmarks.part(40).x, landmarks.part(40).y),\n",
    "                            (landmarks.part(41).x, landmarks.part(41).y)], np.int32)\n",
    "        \n",
    "        #cv2.polylines(frame, [left_eye_region], True, 255, 2)\n",
    "        height, width, _ = frame.shape\n",
    "    \n",
    "        #create the mask to extract xactly the inside of the left eye and exclude all the sorroundings.\n",
    "        mask = np.zeros((height, width), np.uint8)\n",
    "        cv2.polylines(mask, [left_eye_region], True, 255, 2)\n",
    "        cv2.fillPoly(mask, [left_eye_region], 255)\n",
    "        left_eye = cv2.bitwise_and(gray,gray,mask = mask)\n",
    "        \n",
    "        #We now extract the eye from the face and we put it on his own window.Onlyt we need to keep in mind that wecan only cut\n",
    "        #out rectangular shapes from the image, so we take all the extremes points of the eyes to get the rectangle\n",
    "        min_x = np.min(left_eye_region[:, 0])\n",
    "        max_x = np.max(left_eye_region[:, 0])\n",
    "        min_y = np.min(left_eye_region[:, 1])\n",
    "        max_y = np.max(left_eye_region[:, 1])\n",
    "        gray_eye = left_eye[min_y: max_y, min_x: max_x]\n",
    "        \n",
    "        #threshold to seperate iris and pupil from the white part of the eye.\n",
    "        _, threshold_eye = cv2.threshold(gray_eye, 70, 255, cv2.THRESH_BINARY)\n",
    "        \n",
    "        threshold_eye = cv2.resize(threshold_eye,None ,fx = 5,fy = 5)\n",
    "        eye = cv2.resize(gray_eye, None,fx = 5,fy = 5)\n",
    "        \n",
    "        cv2.imshow(\"EYE\",eye)\n",
    "        cv2.imshow(\"THRESHOLD\",threshold_eye)\n",
    "        cv2.imshow(\"LEFT_EYE\",left_eye)\n",
    "        cv2.imshow(\"mask\",mask)\n",
    "         \n",
    "\n",
    "    cv2.imshow(\"Frame\",frame)\n",
    "    \n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    #close the webcam when escape key is pressed\n",
    "    if key == 27:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Screen Shot Of The Output\n",
    "\n",
    "To get the clear picture of what is going on ,i have added the screen shot for the output of the code.\n",
    "\n",
    "##### For the mask\n",
    "\n",
    "<img src=\"picture/mask.png\">\n",
    "\n",
    "##### For The Left_Eye\n",
    "\n",
    "<img src=\"picture/left_eye.png\">\n",
    "\n",
    "\n",
    "##### For The Eye\n",
    "\n",
    "<img src=\"picture/eye.png\">\n",
    "\n",
    "##### For The threshold\n",
    "\n",
    "<img src=\"picture/threshold.png\">\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ShubhangiDabral13..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
